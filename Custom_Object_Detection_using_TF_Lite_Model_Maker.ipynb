{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Custom Object Detection using TF Lite Model Maker.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "3f06a60cda354d81c2225d2767ee5b44cb4579f2789781878c56ddc9e6e490bb"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('detectron2': conda)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# **Custom Object Detection using TF Lite Model Maker**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvAObmTqglq"
      },
      "source": [
        "#### **Step 1: Install the required packages**\n",
        "Install TF Lite Model Maker to build the object detection model and Pycocotools for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhl8lqVamEty"
      },
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q pycocotools"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6lRhVK9Q_0U"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import PIL.Image\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRd13bfetO7B"
      },
      "source": [
        "#### **Step 2: Prepare the dataset** (Optional and to be done on your local machine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Ccvw-ismsT"
      },
      "source": [
        "#### **(i) Convert images to JPEG**\n",
        "The images in your dataset should be of the format `.jpeg`. Sometimes, it is possible that although the extension of the images in your dataset might be `.jpg` or `.jpeg`, its actual format is something else. For example, `image.jpg` might be a `PNG` image and not `JPEG`.\n",
        "\n",
        "Use this [script](https://github.com/NSTiwari/Custom-Object-Detection-on-Android-using-TF-Lite/blob/master/convert_images_to_jpeg.py) to programatically convert all the images in your dataset in the `JPEG` format just to be sure else you'll face an error ahead.\n",
        "\n",
        "**Note:** Do this cleaning and conversion on your local machine before you upload it on Kaggle/Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkVil2mMhVpg"
      },
      "source": [
        "#### **(ii) Check image format**\n",
        "Verfiy the image format after they're converted into `JPEG`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqgZhhADv3MZ"
      },
      "source": [
        "img = PIL.Image.open('/content/cartoon-detection/train/images/cartoon57.jpg')\n",
        "img.format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD8T8Iqkkz9v"
      },
      "source": [
        "#### **Step 3: Download dataset from Kaggle/Google Drive**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5CVL9n_kPwt"
      },
      "source": [
        "Fetch the dataset which you've uploaded on Kaggle. You can also use Google Drive to store your dataset and fetch it by mounting Google Drive on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRUVNoglkeNF"
      },
      "source": [
        "# Install Kaggle API\n",
        "!pip install -q kaggle\n",
        "!pip install -q kaggle-cli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyD86eNokk8c"
      },
      "source": [
        "# only for Google Colab\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"<your_kaggle_username>\" \n",
        "os.environ['KAGGLE_KEY'] = \"<your_kaggle_key>\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JiaGyCekswg"
      },
      "source": [
        "!kaggle datasets download -d nstiwari/cartoondetection --unzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5M8iuydhVae"
      },
      "source": [
        "#### **Step 4: Choose an object detection model architecture to train your model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xushUyZXqP59"
      },
      "source": [
        "This tutorial uses the EfficientDet-Lite2 model. EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture. \n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 260           | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Pixel 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtdZ-JDwMimd"
      },
      "source": [
        "spec = model_spec.get('efficientdet_lite2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5U-A3tw6Y27"
      },
      "source": [
        "#### **Step 5: Load the dataset.**\n",
        "\n",
        "Load the train and test dataset by passing the `images_dir`, `annotations_dir` and `labels` of the dataset as the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7wwyI2gLKns"
      },
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\"/content/cartoon-detection/train/images\", \"/content/cartoon-detection/train/annotations\", ['doraemon', 'mrbean', 'scooby', 'mickey', 'mcqueen'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ta9DmrSQNRMN"
      },
      "source": [
        "validation_data = object_detector.DataLoader.from_pascal_voc('cartoon-detection/test/images', 'cartoon-detection/test/annotations', ['doraemon', 'mrbean', 'scooby', 'mickey', 'mcqueen'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uZkLR6N6gDR"
      },
      "source": [
        "#### **Step 6: Train the TensorFlow model with the training data.**\n",
        "Start the model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwlYdTcg63xy"
      },
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=4, train_whole_model=True, validation_data=validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BzCHLWJ6h7q"
      },
      "source": [
        "#### **Step 7: Evaluate the model with the test data.**\n",
        "\n",
        "After training the object detection model using the images in the training dataset, evalutate the model on the test data to see how it performs against the data it has never seen before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xmnl6Yy7ARn"
      },
      "source": [
        "model.evaluate(validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCDMe0e6jlT"
      },
      "source": [
        "#### **Step 8: Export as a TensorFlow Lite model.**\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is full integer quantization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm_UULdW7A9T"
      },
      "source": [
        "model.export(export_dir='.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UygYErfCD5m3"
      },
      "source": [
        "#### **Step 9: Evaluate the TensorFlow Lite model.**\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop. \n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHYDWcljr6jq"
      },
      "source": [
        "model.evaluate_tflite('model.tflite', validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVxaf3x_7OfB"
      },
      "source": [
        "You can download the TensorFlow Lite model file using the left sidebar of Colab. Right-click the `model.tflite` file and choose `Download` to download it to your local computer.\n",
        "\n",
        "After training the model you can use the [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview) to [integrate the object detector into an Android application](https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector)."
      ]
    }
  ]
}